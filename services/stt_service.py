from pydub import AudioSegment
from openai import OpenAI
import os
from dotenv import load_dotenv

# ğŸ“¦ .env í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
dotenv_path = os.path.join(os.path.dirname(__file__), '..', '..', '.env')
load_dotenv(dotenv_path)

# ğŸ”‘ OpenAI API Key ë¡œë“œ ë° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
openai_key = os.getenv("OPENAI_API_KEY")
if not openai_key:
    raise ValueError("âŒ OPENAI_API_KEYê°€ .envì— ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
client = OpenAI(api_key=openai_key)

# ğŸ”„ ì˜¤ë””ì˜¤ í¬ë§·ì„ Whisper API í˜¸í™˜ wavë¡œ ë³€í™˜
def convert_audio_to_wav(input_path: str, output_path: str):
    """
    ì…ë ¥ ì˜¤ë””ì˜¤ íŒŒì¼ì„ 16kHz, mono wav í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    """
    audio = AudioSegment.from_file(input_path)
    audio = audio.set_frame_rate(16000).set_channels(1)
    audio.export(output_path, format="wav")

# ğŸ§  OpenAI Whisper APIë¥¼ í†µí•œ STT ìˆ˜í–‰
def transcribe_audio_file(file_path: str) -> str:
    """
    Whisper APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ ì˜¤ë””ì˜¤ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ì „ì‚¬í•¨
    """
    with open(file_path, "rb") as f:
        transcript = client.audio.transcriptions.create(
            model="whisper-1",
            file=f,
            response_format="text"  # ë˜ëŠ” "json"
        )
    return transcript.strip()

# from pydub import AudioSegment
# import whisper
# from pyannote.audio import Pipeline
# import os
# from dotenv import load_dotenv
#
# # Whisper ëª¨ë¸ ë¡œë“œ
# model = whisper.load_model("small")
#
# # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# dotenv_path = os.path.join(os.path.dirname(__file__), '..', '..', '.env')
# #load_dotenv()
# load_dotenv(dotenv_path)
# HUGGINGFACE_TOKEN = os.getenv("HF_TOKEN")
# print(f"HUGGINGFACE_TOKEN: {HUGGINGFACE_TOKEN[:8]}..." if HUGGINGFACE_TOKEN else "âŒ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ì‹¤íŒ¨")
#
# # pyannote í™”ì ë¶„ë¦¬ íŒŒì´í”„ë¼ì¸
# diarization_pipeline = Pipeline.from_pretrained(
#     "pyannote/speaker-diarization",
#     use_auth_token=HUGGINGFACE_TOKEN
# )
#
# def convert_webm_to_wav(webm_path, wav_path):
#     audio = AudioSegment.from_file(webm_path, format="webm")
#     audio.export(wav_path, format="wav")
# # ì˜¤ë””ì˜¤ ë³€í™˜ (í™•ì¥ì ë¬´ê´€)
# def convert_audio_to_wav(input_path: str, output_path: str):
#     audio = AudioSegment.from_file(input_path)
#     audio = audio.set_frame_rate(16000).set_channels(1)  # ìƒ˜í”Œë ˆì´íŠ¸ ë° ì±„ë„ ì„¤ì •
#     audio.export(output_path, format="wav")
#
# # Whisper ë‹¨ìˆœ ì „ì‚¬
# def transcribe_audio_file(file_path: str) -> str:
#     result = model.transcribe(file_path)
#     return result["text"]
#
# # Whisper + pyannote í™”ì ë¶„ë¦¬ ì „ì‚¬
# def transcribe_audio_file_with_speaker_labels(wav_path: str) -> list:
#     diarization = diarization_pipeline(wav_path)
#     result = model.transcribe(wav_path, verbose=False)
#
#     segments = []
#     for turn in diarization.itertracks(yield_label=True):
#         start, end = turn[0].start, turn[0].end
#         speaker = turn[2]
#         spoken_texts = [
#             seg['text'] for seg in result['segments']
#             if not (seg['end'] < start or seg['start'] > end)
#         ]
#         combined = ' '.join(spoken_texts).strip()
#         if combined:
#             segments.append({
#                 "speaker": speaker,
#                 "start": round(start, 1),
#                 "end": round(end, 1),
#                 "text": combined
#             })
#     return segments