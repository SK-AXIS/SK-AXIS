# app/services/pipeline/pipeline_agents.py

from datetime import datetime
import json
from app.schemas.state import InterviewState
from app.schemas.nonverbal import FacialExpression
from app.services.interview.rewrite_service import rewrite_answer
from app.services.interview.evaluation_service import evaluate_keywords_from_full_answer
from app.services.interview.nonverbal_service import evaluate
from app.constants.evaluation_constants_full_all import (
    EVAL_CRITERIA_WITH_ALL_SCORES,
    TECHNICAL_EVAL_CRITERIA_WITH_ALL_SCORES,
    DOMAIN_EVAL_CRITERIA_WITH_ALL_SCORES
)
from .pipeline_chains import chains
from .pipeline_utils import utils, KST

async def rewrite_agent(state: InterviewState) -> InterviewState:
    """
    STT ê²°ê³¼ë¥¼ ë¬¸ë²•ì ìœ¼ë¡œ ì •ì œí•˜ëŠ” ë¦¬ë¼ì´íŒ… ì—ì´ì „íŠ¸
    
    Args:
        state (InterviewState): ë©´ì ‘ ìƒíƒœ ê°ì²´
        
    Returns:
        InterviewState: ë¦¬ë¼ì´íŒ… ê²°ê³¼ê°€ ì¶”ê°€ëœ ìƒíƒœ
        
    ì²˜ë¦¬ ê³¼ì •:
    1. STT ê²°ê³¼ì—ì„œ ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ
    2. GPT-4o-minië¡œ ì˜ë¯¸ ë³´ì¡´ ê¸°ë°˜ í…ìŠ¤íŠ¸ ì •ì œ
    3. ì¬ì‹œë„ íšŸìˆ˜ ê´€ë¦¬ (í˜„ì¬ ì¬ì‹œë„ ë¹„í™œì„±í™”)
    4. ê²°ê³¼ë¥¼ state["rewrite"]["items"]ì— ì €ì¥
    
    Note:
        - ì§€ì›ì ë‹µë³€ ì˜ë¯¸ ì ˆëŒ€ ë³€ê²½ ì•ˆ í•¨
        - ë¬¸ë²• ì˜¤ë¥˜ ë° ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        - ë©´ì ‘ê´€ ë°œì–¸ í•„í„°ë§
        - GPT-4o-mini ì‚¬ìš©ìœ¼ë¡œ ë¹„ìš© ì ˆì•½
    """
    print("[LangGraph] âœï¸ rewrite_agent ì§„ì…")
    stt = utils.safe_get(state, "stt", {}, context="rewrite_agent")
    stt_segments = utils.safe_get(stt, "segments", [], context="rewrite_agent")
    
    if not stt_segments:
        print("[LangGraph] âš ï¸ STT ì„¸ê·¸ë¨¼íŠ¸ê°€ ì—†ìŒ")
        return state
    
    # ë§ˆì§€ë§‰ STT ì„¸ê·¸ë¨¼íŠ¸ í™•ì¸
    last_segment = stt_segments[-1]
    raw = last_segment.get("raw", "ì—†ìŒ")
    
    # ë¬´ì˜ë¯¸í•œ STT ê²°ê³¼ì¸ì§€ í™•ì¸
    if last_segment.get("meaningless", False):
        print(f"[LangGraph] ğŸš« ë¬´ì˜ë¯¸í•œ STT ê²°ê³¼ - ë¦¬ë¼ì´íŒ… ê±´ë„ˆëœ€: {raw}")
        utils.add_decision_log(state, "rewrite_agent", "skipped", {
            "reason": "meaningless_stt_result",
            "raw": raw
        })
        return state
    
    if not raw or not str(raw).strip():
        raw = "ì—†ìŒ"
    
    rewritten, _ = await rewrite_answer(raw)
    if not rewritten or not str(rewritten).strip():
        rewritten = "ì—†ìŒ"
    item = {"raw": raw, "rewritten": rewritten}

    prev = utils.safe_get(state, "rewrite", {}, context="rewrite_agent")
    prev_retry = utils.safe_get(prev, "retry_count", 0, context="rewrite_agent")
    prev_force = utils.safe_get(prev, "force_ok", False, context="rewrite_agent")
    prev_final = utils.safe_get(prev, "final", [], context="rewrite_agent")

    # retry_countê°€ 3 ì´ìƒì´ë©´ ë” ì´ìƒ ì¦ê°€ì‹œí‚¤ì§€ ì•ŠìŒ
    if prev_retry >= 3:
        retry_count = 3
        force = True
    else:
        retry_count = prev_retry + 1
        force = prev_force

    state["rewrite"] = {
        **prev,
        "done": False,
        "items": prev.get("items", []) + [item],
        "retry_count": retry_count,
        "force_ok": force,
        "final": prev_final
    }

    utils.add_decision_log(state, "rewrite_agent", "done", {
        "retry_count": retry_count,
        "force": force
    })

    return state

async def rewrite_judge_agent(state: InterviewState) -> InterviewState:
    """
    ë¦¬ë¼ì´íŒ… ê²°ê³¼ë¥¼ ê²€ì¦í•˜ëŠ” íŒì • ì—ì´ì „íŠ¸
    
    Args:
        state (InterviewState): ë©´ì ‘ ìƒíƒœ ê°ì²´
        
    Returns:
        InterviewState: ê²€ì¦ ê²°ê³¼ê°€ ì¶”ê°€ëœ ìƒíƒœ
        
    ì²˜ë¦¬ ê³¼ì •:
    1. ë¦¬ë¼ì´íŒ… ê²°ê³¼ í’ˆì§ˆ ê²€ì¦ (ì˜ë¯¸ ë³´ì¡´, ë¬¸ë²• ì •í™•ì„±)
    2. GPT-4o-minië¡œ íŒì • ìˆ˜í–‰
    3. ê²€ì¦ í†µê³¼ ì‹œ final ë°°ì—´ì— ì¶”ê°€
    4. ê°•ì œ í†µê³¼ í”Œë˜ê·¸ ì²˜ë¦¬
    
    Note:
        - í˜„ì¬ ì¬ì‹œë„ ë¡œì§ ë¹„í™œì„±í™”ë¡œ ëŒ€ë¶€ë¶„ í†µê³¼
        - JSON íŒŒì‹± ì˜¤ë¥˜ ì‹œ ì•ˆì „ ì²˜ë¦¬
        - ì¤‘ë³µ ë‹µë³€ í•„í„°ë§ ë¡œì§ í¬í•¨
    """
    print("[LangGraph] ğŸ§ª rewrite_judge_agent ì§„ì…")
    rewrite = utils.safe_get(state, "rewrite", {}, context="rewrite_judge_agent")
    items   = utils.safe_get(rewrite, "items", [])
    force   = utils.safe_get(rewrite, "force_ok", False, context="rewrite_judge_agent")

    if not items:
        utils.add_decision_log(state, "rewrite_judge_agent", "error", {
            "error": "No rewrite items found"
        })
        return state

    for item in items:
        if "ok" in item:
            continue

        try:
            # ì²´ì´ë‹ ì‚¬ìš©
            result = await chains.rewrite_judge_chain.ainvoke({
                "raw": item["raw"],
                "rewritten": item["rewritten"]
            })
            
            item["ok"] = result.get("ok", False)
            item["judge_notes"] = result.get("judge_notes", [])

            print(f"[DEBUG] ğŸ“Š íŒì • ê²°ê³¼: ok={item['ok']}, notes={item['judge_notes']}")

            # ê°•ì œ í†µê³¼ í”Œë˜ê·¸ ì²˜ë¦¬
            if not item["ok"] and force:
                print("âš ï¸ rewrite ì‹¤íŒ¨ í•­ëª© ê°•ì œ ok ì²˜ë¦¬ë¨")
                item["ok"] = True
                item["judge_notes"].append("ê°•ì œ í†µê³¼ (ì¬ì‹œë„ 3íšŒ ì´ˆê³¼)")

            if item["ok"]:
                # ì¤‘ë³µëœ rewritten ë‹µë³€ì´ ì´ë¯¸ finalì— ìˆìœ¼ë©´ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
                rewritten = item["rewritten"]
                if not any(f["rewritten"] == rewritten for f in rewrite.get("final", [])):
                    rewrite.setdefault("final", []).append({
                        "raw":       item["raw"],
                        "rewritten": rewritten,
                        "timestamp": datetime.now(KST).isoformat()
                    })
                else:
                    pass

            # ê°•ì œ í†µê³¼ í”Œë˜ê·¸ê°€ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ finalì— ì¶”ê°€
            if force and not item.get("ok", False):
                print("âš ï¸ ê°•ì œ í†µê³¼ í”Œë˜ê·¸ë¡œ ì¸í•´ finalì— ì¶”ê°€")
                rewritten = item["rewritten"]
                if not any(f["rewritten"] == rewritten for f in rewrite.get("final", [])):
                    rewrite.setdefault("final", []).append({
                        "raw":       item["raw"],
                        "rewritten": rewritten,
                        "timestamp": datetime.now(KST).isoformat()
                    })
                else:
                    pass
                item["ok"] = True
                item["judge_notes"].append("ê°•ì œ í†µê³¼ (ì¬ì‹œë„ 3íšŒ ì´ˆê³¼)")

            utils.add_decision_log(state, "rewrite_judge_agent", f"ok={item['ok']}", {
                "notes": item["judge_notes"]
            })
            print(f"[LangGraph] âœ… íŒì • ê²°ê³¼: ok={item['ok']}")

        except Exception as e:
            print(f"[ERROR] rewrite_judge_agent ì²´ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            item["ok"] = False
            item["judge_notes"] = [f"judge error: {e}"]
            utils.add_decision_log(state, "rewrite_judge_agent", "error", {
                "error": str(e)
            })

    # ë§ˆì§€ë§‰ í•­ëª©ì´ ok=Trueë©´ ì™„ë£Œ í‘œì‹œ
    if rewrite["items"][-1].get("ok", False):
        rewrite["done"] = True
    return state

async def nonverbal_evaluation_agent(state: InterviewState) -> InterviewState:
    """
    ë¹„ì–¸ì–´ì  ìš”ì†Œ(í‘œì •)ë¥¼ í‰ê°€í•˜ëŠ” ì—ì´ì „íŠ¸
    
    Args:
        state (InterviewState): ë©´ì ‘ ìƒíƒœ ê°ì²´
        
    Returns:
        InterviewState: ë¹„ì–¸ì–´ì  í‰ê°€ ê²°ê³¼ê°€ ì¶”ê°€ëœ ìƒíƒœ
        
    ì²˜ë¦¬ ê³¼ì •:
    1. nonverbal_countsì—ì„œ í‘œì • ë°ì´í„° ì¶”ì¶œ
    2. FacialExpression ê°ì²´ë¡œ ë³€í™˜
    3. GPT-4o-minië¡œ í‘œì • íŒ¨í„´ ë¶„ì„
    4. 0.0~1.0 ì ìˆ˜ë¥¼ 15ì  ë§Œì ìœ¼ë¡œ í™˜ì‚°
    5. ê²°ê³¼ë¥¼ state["evaluation"]["results"]["ë¹„ì–¸ì–´ì "]ì— ì €ì¥
    
    Note:
        - í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ìˆ˜ì§‘ëœ smile, neutral, frown, angry íšŸìˆ˜ ê¸°ë°˜
        - ì ì ˆí•œ í‘œì • ë³€í™”ì™€ ì›ƒìŒì€ ê¸ì •ì  í‰ê°€
        - ë°ì´í„° ëˆ„ë½ ì‹œ ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥
    """
    # í‰ê°€ ì‹œì‘ ì‹œê°„ ê¸°ë¡
    evaluation_start_time = datetime.now(KST).timestamp()
    state["_evaluation_start_time"] = evaluation_start_time
    print(f"[â±ï¸] í‰ê°€ ì‹œì‘: {datetime.now(KST).strftime('%Y-%m-%d %H:%M:%S')}")
    
    ts = datetime.now(KST).isoformat()
    try:
        counts = utils.safe_get(state, "nonverbal_counts", {}, context="nonverbal_evaluation_agent")
        print(f"[DEBUG] nonverbal_counts: {counts}")
        # êµ¬ì¡° ì²´í¬
        if not counts or not isinstance(counts, dict):
            print("[WARNING] nonverbal_countsê°€ dictê°€ ì•„ë‹˜ ë˜ëŠ” ë¹„ì–´ìˆìŒ. ë¹„ì–¸ì–´ì  í‰ê°€ë¥¼ ê±´ë„ˆëœ€.")
            state.decision_log.append("Nonverbal data not available for evaluation.")
            return state
        if "expression" not in counts or not isinstance(counts["expression"], dict):
            print("[WARNING] nonverbal_counts['expression']ê°€ dictê°€ ì•„ë‹˜ ë˜ëŠ” ì—†ìŒ. ë¹„ì–¸ì–´ì  í‰ê°€ë¥¼ ê±´ë„ˆëœ€.")
            state.decision_log.append("Nonverbal expression data not available for evaluation.")
            return state
        # expression ë‚´ë¶€ í‚¤ ì²´í¬
        exp = counts["expression"]
        required_keys = ["smile", "neutral", "frown", "angry"]
        for k in required_keys:
            if k not in exp or not isinstance(exp[k], int):
                print(f"[WARNING] nonverbal_counts['expression']ì— {k}ê°€ ì—†ê±°ë‚˜ intê°€ ì•„ë‹˜: {exp}")
        facial = FacialExpression.parse_obj(exp)
        print(f"[DEBUG] facial_expression: {facial}")
        res = await evaluate(facial)
        score = res.get("score", 0)
        analysis = res.get("analysis", "")
        feedback = res.get("feedback", "")
        # print(f"[DEBUG] ë¹„ì–¸ì–´ì  í‰ê°€ ê²°ê³¼(score): {score}, analysis: {analysis}, feedback: {feedback}")
        pts = int(round(score * 15))
        if pts == 0:
            print("[WARNING] ë¹„ì–¸ì–´ì  í‰ê°€ ì ìˆ˜ê°€ 0ì…ë‹ˆë‹¤. í”„ë¡ íŠ¸/ë°ì´í„° ì „ë‹¬/LLM í”„ë¡¬í”„íŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        evaluation = utils.safe_get(state, "evaluation", {}, context="nonverbal_evaluation_agent")
        results = utils.safe_get(evaluation, "results", {}, context="nonverbal_evaluation_agent")
        results["ë¹„ì–¸ì–´ì "] = {"score": pts, "reason": analysis or feedback or "í‰ê°€ ì‚¬ìœ ì—†ìŒ"}
        state.setdefault("evaluation", {}).setdefault("results", {})["ë¹„ì–¸ì–´ì "] = {
            "score": pts,
            "reason": analysis or feedback or "í‰ê°€ ì‚¬ìœ ì—†ìŒ"
        }
        utils.add_decision_log(state, "nonverbal_evaluation", "success", {
            "score": pts
        })
        print(f"[DEBUG] nonverbal_evaluation_agent - state['evaluation']['results']['ë¹„ì–¸ì–´ì ']: {state.get('evaluation', {}).get('results', {}).get('ë¹„ì–¸ì–´ì ')}")
    except Exception as e:
        print(f"[ERROR] ë¹„ì–¸ì–´ì  í‰ê°€ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        utils.add_decision_log(state, "nonverbal_evaluation", "error", {
            "error": str(e)
        })
    return state

async def evaluation_agent(state: InterviewState) -> InterviewState:
    """
    í‚¤ì›Œë“œë³„ í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì—ì´ì „íŠ¸
    
    Args:
        state (InterviewState): ë©´ì ‘ ìƒíƒœ ê°ì²´
        
    Returns:
        InterviewState: í‰ê°€ ê²°ê³¼ê°€ ì¶”ê°€ëœ ìƒíƒœ
        
    ì²˜ë¦¬ ê³¼ì •:
    1. ë¦¬ë¼ì´íŒ…ëœ ë‹µë³€ë“¤ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
    2. 8ê°œ í‚¤ì›Œë“œ Ã— 3ê°œ ê¸°ì¤€ = 24ê°œ í•­ëª© í‰ê°€
    3. í‰ê°€ ê²°ê³¼ ì •ê·œí™” ë° ë³‘í•©
    4. ì¬ì‹œë„ íšŸìˆ˜ ê´€ë¦¬
    
    Note:
        - GPT-4o-mini ì‚¬ìš©ìœ¼ë¡œ ë¹„ìš© ì ˆì•½
        - í‰ê°€ ê²°ê³¼ ì •ê·œí™”ë¡œ ì¼ê´€ì„± í™•ë³´
        - ë¹„ì–¸ì–´ì  í‰ê°€ ê²°ê³¼ì™€ ë³‘í•©
    """
    print("[LangGraph] ğŸ§  evaluation_agent ì§„ì…")
    
    # ë¦¬ë¼ì´íŒ…ëœ ë‹µë³€ë“¤ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
    rewrite = utils.safe_get(state, "rewrite", {}, context="evaluation_agent:rewrite")
    final_items = utils.safe_get(rewrite, "final", [], context="evaluation_agent:rewrite.final")
    
    if not final_items:
        print("[WARNING] final_itemsê°€ ë¹„ì–´ìˆìŒ. í‰ê°€ë¥¼ ê±´ë„ˆëœ€.")
        utils.add_decision_log(state, "evaluation_agent", "error", {
            "error": "No final items found"
        })
        return state
    
    full_answer = "\n".join(item["rewritten"] for item in final_items)
    print(f"[DEBUG] í‰ê°€ ëŒ€ìƒ ë‹µë³€: {full_answer[:100]}...")

    # í‰ê°€ ê¸°ì¤€ í‚¤ ëª©ë¡ì„ ê°€ì ¸ì˜´
    all_criteria = {**EVAL_CRITERIA_WITH_ALL_SCORES, **TECHNICAL_EVAL_CRITERIA_WITH_ALL_SCORES, **DOMAIN_EVAL_CRITERIA_WITH_ALL_SCORES}

    # í‰ê°€ ê²°ê³¼ë¥¼ ì •ì œí•˜ëŠ” í•¨ìˆ˜ (quotes í•„ë“œê¹Œì§€ ë³´ì¥)
    def normalize_results(results):
        normalized = {}
        for keyword, criteria in all_criteria.items():
            kw_result = results.get(keyword, {}) if isinstance(results, dict) else {}
            normalized[keyword] = {}
            for crit_name in criteria.keys():
                val = kw_result.get(crit_name) if isinstance(kw_result, dict) else None
                # ë³´ê°•: dictê°€ ì•„ë‹ˆë©´ ë¬´ì¡°ê±´ dictë¡œ ê°ì‹¸ê¸°
                if not isinstance(val, dict):
                    if isinstance(val, int):
                        val = {"score": val, "reason": "í‰ê°€ ì‚¬ìœ ì—†ìŒ", "quotes": []}
                    else:
                        val = {"score": 1, "reason": "í‰ê°€ ì‚¬ìœ ì—†ìŒ", "quotes": []}
                score = val.get("score", 1)
                reason = val.get("reason", "í‰ê°€ ì‚¬ìœ ì—†ìŒ")
                quotes = val.get("quotes", [])
                if not isinstance(quotes, list):
                    quotes = []
                normalized[keyword][crit_name] = {
                    "score": score,
                    "reason": reason,
                    "quotes": quotes
                }
        # ë¹„ì–¸ì–´ì  ìš”ì†Œë„ í•­ìƒ dictë¡œ ë³´ì •
        if "ë¹„ì–¸ì–´ì " in results:
            nonverbal = results["ë¹„ì–¸ì–´ì "]
            if not isinstance(nonverbal, dict):
                if isinstance(nonverbal, int):
                    nonverbal = {"score": nonverbal, "reason": "í‰ê°€ ì‚¬ìœ ì—†ìŒ", "quotes": []}
                else:
                    nonverbal = {"score": 1, "reason": "í‰ê°€ ì‚¬ìœ ì—†ìŒ", "quotes": []}
            if "quotes" not in nonverbal or not isinstance(nonverbal["quotes"], list):
                nonverbal["quotes"] = []
            normalized["ë¹„ì–¸ì–´ì "] = nonverbal
        return normalized

    results = await evaluate_keywords_from_full_answer(full_answer)
    results = normalize_results(results)

    prev_eval = utils.safe_get(state, "evaluation", {}, context="evaluation_agent:evaluation")
    prev_results = prev_eval.get("results", {})
    # ê¸°ì¡´ ë¹„ì–¸ì–´ì  ë“± ê²°ê³¼ì™€ ìƒˆ í‰ê°€ ê²°ê³¼ ë³‘í•©
    merged_results = {**prev_results, **results}
    prev_retry = utils.safe_get(prev_eval, "retry_count", 0, context="evaluation_agent:evaluation.retry_count")
    if "ok" in prev_eval and utils.safe_get(prev_eval, "ok", context="evaluation_agent:evaluation.ok") is False:
        retry_count = prev_retry + 1
    else:
        retry_count = prev_retry

    state["evaluation"] = {
        **prev_eval,
        "done": True,
        "results": merged_results,
        "retry_count": retry_count,
        "ok": False  # íŒì • ì „ì´ë¯€ë¡œ Falseë¡œ ì´ˆê¸°í™”
    }
    state["done"] = True  # íŒŒì´í”„ë¼ì¸ ì „ì²´ ì¢…ë£Œ ì‹ í˜¸ ì¶”ê°€
    utils.add_decision_log(state, "evaluation_agent", "done", {
        "retry_count": retry_count
    })

    return state

async def evaluation_judge_agent(state: InterviewState) -> InterviewState:
    """
    í‰ê°€ ê²°ê³¼ë¥¼ ê²€ì¦í•˜ëŠ” íŒì • ì—ì´ì „íŠ¸
    
    Args:
        state (InterviewState): ë©´ì ‘ ìƒíƒœ ê°ì²´
        
    Returns:
        InterviewState: ê²€ì¦ ê²°ê³¼ê°€ ì¶”ê°€ëœ ìƒíƒœ
        
    ê²€ì¦ í•­ëª©:
    1. êµ¬ì¡°ì  ê²€ì¦: ê° í‚¤ì›Œë“œë‹¹ 3ê°œ ê¸°ì¤€ ì¡´ì¬ ì—¬ë¶€
    2. ì ìˆ˜ ë²”ìœ„ ê²€ì¦: 1~5ì  ë²”ìœ„ ë‚´ ì ìˆ˜ í™•ì¸
    3. ì´ì  ê²€ì¦: ìµœëŒ€ ì ìˆ˜ ì´ˆê³¼ ì—¬ë¶€ í™•ì¸
    4. ë‚´ìš© ê²€ì¦: GPT-4o-minië¡œ í‰ê°€ ë‚´ìš© íƒ€ë‹¹ì„± ê²€ì¦
    
    Note:
        - ê²€ì¦ ì‹¤íŒ¨ ì‹œì—ë„ ì¬ì‹œë„ ì œí•œìœ¼ë¡œ ì§„í–‰
        - ë‚´ìš© ê²€ì¦ ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ í†µê³¼ ì²˜ë¦¬
        - ëª¨ë“  ê²€ì¦ ê²°ê³¼ë¥¼ decision_logì— ê¸°ë¡
    """
    evaluation = utils.safe_get(state, "evaluation", {}, context="evaluation_judge_agent:evaluation")
    results = utils.safe_get(evaluation, "results", {}, context="evaluation_judge_agent:evaluation.results")
    if not results:
        utils.add_decision_log(state, "evaluation_judge_agent", "error", {
            "error": "No evaluation results found"
        })
        print("[judge] No evaluation results found, will stop.")
        state["evaluation"]["ok"] = True  # ë” ì´ìƒ retry/continue ì•ˆ í•˜ë„ë¡ Trueë¡œ ì„¤ì •
        return state

    judge_notes = []
    is_valid = True

    # 1. í•­ëª© ìˆ˜ ê²€ì¦ (ê° í‚¤ì›Œë“œì— 3ê°œ)
    for kw, criteria in results.items():
        if len(criteria) != 3:
            judge_notes.append(f"Keyword '{kw}' has {len(criteria)} criteria (expected 3)")
            is_valid = False

    # 2. ì ìˆ˜ ë²”ìœ„ ê²€ì¦ (1~5)
    for criteria in results.values():
        for data in criteria.values():
            # print(f"[DEBUG] evaluation_judge_agent - data type: {type(data)}, value: {data}")
            if isinstance(data, dict):
                s = data.get("score", 0)
            elif isinstance(data, int):
                s = data
            else:
                s = 0
            if not (1 <= s <= 5):
                judge_notes.append(f"Invalid score {s}")
                is_valid = False

    # 3. ì´ì  ê²€ì¦
    total = 0
    for crit in results.values():
        for c in crit.values():
            if isinstance(c, dict):
                total += c.get("score", 0)
            elif isinstance(c, int):
                total += c
            else:
                total += 0
    max_score = len(results) * 3 * 5
    if total > max_score:
        judge_notes.append(f"Total {total} exceeds max {max_score}")
        is_valid = False

    print(f"[judge] is_valid={is_valid}, judge_notes={judge_notes}, total={total}, max_score={max_score}")
    state["evaluation"]["judge"] = {
        "ok": is_valid,
        "judge_notes": judge_notes,
        "total_score": total,
        "max_score": max_score
    }
    state["evaluation"]["ok"] = is_valid

    # === ë‚´ìš© ê²€ì¦ LLM í˜¸ì¶œ ì¶”ê°€ ===
    try:
        final_items = utils.safe_get(state, "rewrite", {}).get("final", [])
        if not final_items:
            # final_itemsê°€ ë¹„ì–´ìˆìœ¼ë©´ raw í…ìŠ¤íŠ¸ ì‚¬ìš©
            stt_segments = state.get("stt", {}).get("segments", [])
            if stt_segments:
                answer = stt_segments[-1].get("raw", "ë‹µë³€ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                answer = "ë‹µë³€ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
        else:
            answer = "\n".join(item["rewritten"] for item in final_items)
            
        evaluation = json.dumps(state.get("evaluation", {}).get("results", {}), ensure_ascii=False)
        criteria = json.dumps({
            **EVAL_CRITERIA_WITH_ALL_SCORES,
            **TECHNICAL_EVAL_CRITERIA_WITH_ALL_SCORES,
            **DOMAIN_EVAL_CRITERIA_WITH_ALL_SCORES
        }, ensure_ascii=False)

        # ì²´ì´ë‹ ì‚¬ìš©
        result = await chains.content_validation_chain.ainvoke({
            "answer": answer,
            "evaluation": evaluation,
            "criteria": criteria
        })
        
        state["evaluation"]["content_judge"] = {
            "ok": result.get("ok", False),
            "judge_notes": result.get("judge_notes", [])
        }
        print(f"[LangGraph] âœ… ë‚´ìš© ê²€ì¦ ê²°ê³¼: ok={result.get('ok', False)}, notes={result.get('judge_notes', [])}")
    except Exception as e:
        print(f"[ERROR] content_validation_chain ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        state["evaluation"]["content_judge"] = {
            "ok": True,  # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ì ìœ¼ë¡œ í†µê³¼
            "judge_notes": [f"content judge error: {e}"]
        }
        print(f"[LangGraph] âŒ ë‚´ìš© ê²€ì¦ ì˜¤ë¥˜: {e}")

    utils.add_decision_log(state, "evaluation_judge_agent", f"ok={is_valid}", {
        "total_score": total,
        "max_score": max_score,
        "notes": judge_notes
    })
    
    return state

async def score_summary_agent(state: InterviewState) -> InterviewState:
    """
    í‰ê°€ ê²€ì¦ í›„ ìµœì¢… ì ìˆ˜ í™˜ì‚° ë° ìš”ì•½ì„ ë‹´ë‹¹í•˜ëŠ” ì—ì´ì „íŠ¸
    
    Args:
        state (InterviewState): ë©´ì ‘ ìƒíƒœ ê°ì²´
        
    Returns:
        InterviewState: ìµœì¢… ìš”ì•½ì´ ì¶”ê°€ëœ ìƒíƒœ
        
    ì²˜ë¦¬ ê³¼ì •:
    1. ì˜ì—­ë³„ ì ìˆ˜ ê³„ì‚° ë° 100ì  ë§Œì  í™˜ì‚°
    2. ì§€ì›ì ë‹µë³€ê³¼ í‰ê°€ ì‚¬ìœ ë¥¼ GPT-4oë¡œ ì¢…í•© ìš”ì•½
    3. í‰ê°€ ì†Œìš”ì‹œê°„ ê³„ì‚° ë° ê¸°ë¡
    4. done í”Œë˜ê·¸ ì„¤ì •ìœ¼ë¡œ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ
    
    ìµœì¢… ê²°ê³¼:
    - ì¸ì„±ì  ìš”ì†Œ: 45% (90ì  â†’ 45ì )
    - ì§ë¬´/ë„ë©”ì¸: 45% (30ì  â†’ 45ì )  
    - ë¹„ì–¸ì–´ì  ìš”ì†Œ: 10% (15ì  â†’ 10ì )
    - ì´ì : 100ì  ë§Œì 
    
    Note:
        - GPT-4o ì‚¬ìš©ìœ¼ë¡œ ê³ í’ˆì§ˆ ìš”ì•½ ìƒì„±
        - í‰ê°€ ì†Œìš”ì‹œê°„ ì¶”ì  ë° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        - ëª¨ë“  ê²°ê³¼ë¥¼ state["summary"]ì— ì €ì¥
    """
    evaluation = utils.safe_get(state, "evaluation", {}, context="score_summary_agent:evaluation")
    evaluation_results = utils.safe_get(evaluation, "results", {}, context="score_summary_agent:evaluation.results")
    # print(f"[DEBUG] í‰ê°€ ê²°ê³¼(evaluation_results): {json.dumps(evaluation_results, ensure_ascii=False, indent=2)}")
    nonverbal = evaluation_results.get("ë¹„ì–¸ì–´ì ", {})
    nonverbal_score = nonverbal.get("score", 0)
    nonverbal_reason = nonverbal.get("reason", "í‰ê°€ ì‚¬ìœ ì—†ìŒ")
    # print(f"[DEBUG] ë¹„ì–¸ì–´ì  í‰ê°€: score={nonverbal_score}, reason={nonverbal_reason}")

    # 100ì  ë§Œì  í™˜ì‚° ì ìˆ˜ ê³„ì‚°
    weights, personality_score_scaled, job_domain_score_scaled, nonverbal_score_scaled = utils.calculate_area_scores(evaluation_results, nonverbal_score)
    verbal_score = personality_score_scaled + job_domain_score_scaled
    # print(f"[DEBUG] verbal_score(ì¸ì„±+ì§ë¬´/ë„ë©”ì¸): {verbal_score}")

    # ì „ì²´ í‚¤ì›Œë“œ í‰ê°€ ì‚¬ìœ  ì¢…í•© (SUPEX, VWBE, Passionate, Proactive, Professional, People, ê¸°ìˆ /ì§ë¬´, ë„ë©”ì¸ ì „ë¬¸ì„±)
    all_keywords = [
        "SUPEX", "VWBE", "Passionate", "Proactive", "Professional", "People",
        "ê¸°ìˆ /ì§ë¬´", "ë„ë©”ì¸ ì „ë¬¸ì„±"
    ]
    reasons = []
    for keyword in all_keywords:
        for crit_name, crit in evaluation_results.items():
            reason = crit.get("reason", "")
            if reason:
                reasons.append(f"{keyword} - {crit_name}: {reason}")
            # print(f"[DEBUG] í‰ê°€ ì‚¬ìœ  ì¶”ì¶œ: {keyword} - {crit_name} - {reason}")
    all_reasons = "\n".join(reasons)
    # print(f"[DEBUG] all_reasons(ì „ì²´ í‰ê°€ ì‚¬ìœ ):\n{all_reasons}")

    # ì§€ì›ì ë‹µë³€ ì¶”ì¶œ
    rewrite = utils.safe_get(state, "rewrite", {}, context="score_summary_agent:rewrite")
    final_items = utils.safe_get(rewrite, "final", [], context="score_summary_agent:rewrite.final")
    if final_items:
        answer = "\n".join(item["rewritten"] for item in final_items)
    else:
        stt = utils.safe_get(state, "stt", {}, context="score_summary_agent:stt")
        stt_segments = utils.safe_get(stt, "segments", [], context="score_summary_agent:stt.segments")
        if stt_segments:
            answer = "\n".join(seg.get("raw", "ë‹µë³€ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.") for seg in stt_segments)
        else:
            answer = "ë‹µë³€ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
    # print(f"[DEBUG] ì§€ì›ì ë‹µë³€(answer):\n{answer}")

    # ì²´ì´ë‹ ì‚¬ìš©í•˜ì—¬ ìš”ì•½ ìƒì„±
    try:
        summary_response = await chains.score_summary_chain.ainvoke({
            "answer": answer,
            "all_reasons": all_reasons
        })
        verbal_reason = summary_response.strip().splitlines()[:8]
        # print(f"[DEBUG] summary_text(LLM ìš”ì•½): {verbal_reason}")
    except Exception as e:
        print(f"[ERROR] score_summary_chain ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        verbal_reason = ["ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."]

    # ê° í‚¤ì›Œë“œë³„ ì´ì  ê³„ì‚°
    keyword_scores = {}
    for keyword, criteria in evaluation_results.items():
        if keyword == "ë¹„ì–¸ì–´ì ":
            continue
        total = 0
        for crit in criteria.values():
            if isinstance(crit, dict):
                total += crit.get("score", 0)
            elif isinstance(crit, int):
                total += crit
        keyword_scores[keyword] = total

    # stateì— ì €ì¥
    state["summary"] = {
        "weights": weights,
        "personality_score": personality_score_scaled,
        "job_domain_score": job_domain_score_scaled,
        "verbal_score": verbal_score,
        "verbal_reason": verbal_reason,
        "nonverbal_score": nonverbal_score_scaled,
        "nonverbal_reason": nonverbal_reason,
        "keyword_scores": keyword_scores,
        "total_score": round(verbal_score + nonverbal_score_scaled, 1)
    }
    print(f"[LangGraph] âœ… ì˜ì—­ë³„ ì ìˆ˜/ìš”ì•½ ì €ì¥: {json.dumps(state['summary'], ensure_ascii=False, indent=2)}")

    # í‰ê°€ ì†Œìš”ì‹œê°„ ê³„ì‚° ë° ì¶œë ¥
    start_time = state.get("_evaluation_start_time")
    if start_time:
        end_time = datetime.now(KST).timestamp()
        total_elapsed = end_time - start_time
        print(f"[â±ï¸] í‰ê°€ ì™„ë£Œ: {datetime.now(KST).strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"[â±ï¸] í‰ê°€ ì†Œìš”ì‹œê°„: {total_elapsed:.2f}ì´ˆ (í‰ê°€ ì‹œì‘ â†’ ì™„ë£Œ)")
        
        # decision_logì—ë„ ê¸°ë¡
        utils.add_decision_log(state, "evaluation_complete", "success", {
            "evaluation_elapsed_seconds": round(total_elapsed, 2),
            "start_time": datetime.fromtimestamp(start_time, KST).isoformat(),
            "end_time": datetime.now(KST).isoformat()
        })
        
        # summaryì—ë„ ì†Œìš”ì‹œê°„ ì •ë³´ ì¶”ê°€
        state["summary"]["evaluation_duration"] = {
            "total_seconds": round(total_elapsed, 2),
            "start_time": datetime.fromtimestamp(start_time, KST).isoformat(),
            "end_time": datetime.now(KST).isoformat()
        }
    else:
        print("[â±ï¸] í‰ê°€ ì‹œì‘ ì‹œê°„ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ - done í”Œë˜ê·¸ ì„¤ì •
    state["done"] = True
    print(f"[LangGraph] âœ… ëª¨ë“  í‰ê°€ ì™„ë£Œ - done í”Œë˜ê·¸ ì„¤ì •")

    return state 