From 09d21a37c067e22811301043626d3bab28abc5a4 Mon Sep 17 00:00:00 2001
From: starwarrior77 <kkk792100@naver.com>
Date: Mon, 16 Jun 2025 16:45:27 +0900
Subject: [PATCH 15/33] =?UTF-8?q?mediapipe=20pose=EC=82=AD=EC=A0=9C=20&=20?=
 =?UTF-8?q?=EB=A9=B4=EC=A0=91=EC=9E=90=EC=88=98=20=EC=B5=9C=EB=8C=80=203?=
 =?UTF-8?q?=EC=9D=B8=20&=20=ED=94=84=EB=A1=A0=ED=8A=B8=EC=97=90=EC=84=9C?=
 =?UTF-8?q?=20=EB=85=B9=EC=9D=8C=20=EA=B8=B0=EB=8A=A5=20=EC=B6=94=EA=B0=80?=
 =?UTF-8?q?=20/=20=EB=A9=B4=EC=A0=91=EC=8B=9C=EC=9E=91=20=EB=B2=84?=
 =?UTF-8?q?=ED=8A=BC=EC=97=90=20=EC=9E=88=EB=8A=94=20api=EA=B8=B0=EB=8A=A5?=
 =?UTF-8?q?=20=EB=AF=B8=EA=B5=AC=ED=98=84=EC=9C=BC=EB=A1=9C=20=EC=9D=B8?=
 =?UTF-8?q?=ED=95=9C=20=ED=85=8C=EC=8A=A4=ED=8A=B8=20=EB=B6=88=EA=B0=80?=
 =?UTF-8?q?=EB=8A=A5?=
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

---
 client/package.json                      |   1 +
 client/src/components/Interview.vue      |  76 +++-
 client/src/components/InterviewSetup.vue |   1 +
 client/src/components/PoseMiniWidget.vue | 494 +++++++++++++----------
 client/src/main.ts                       |   1 +
 client/vite.config.ts                    |   9 +-
 6 files changed, 360 insertions(+), 222 deletions(-)

diff --git a/client/package.json b/client/package.json
index 2cf6959..0850880 100644
--- a/client/package.json
+++ b/client/package.json
@@ -20,6 +20,7 @@
     "vue-router": "^4.2.4"
   },
   "devDependencies": {
+    "@tailwindcss/postcss": "^4.1.10",
     "@types/node": "^20.5.7",
     "@vitejs/plugin-vue": "^4.3.4",
     "@vue/eslint-config-typescript": "^12.0.0",
diff --git a/client/src/components/Interview.vue b/client/src/components/Interview.vue
index fbcc9df..e3a9b16 100644
--- a/client/src/components/Interview.vue
+++ b/client/src/components/Interview.vue
@@ -75,7 +75,11 @@
     <div
       class="fixed bottom-4 right-4 bg-gray-900 rounded-lg overflow-hidden shadow-lg z-50 flex items-center justify-center"
       style="width:480px; aspect-ratio:4/3; pointer-events:none;">
-      <PoseMiniWidget style="width:100%; height:100%;" />
+      <PoseMiniWidget 
+        :intervieweeNames="candidates"
+        :intervieweeIds="candidateIds"
+        style="width:100%; height:100%;" 
+      />
     </div>
 
     <!-- AI 로딩 모달 -->
@@ -91,27 +95,19 @@ import { getQuestionsForCandidate as getCandidateQuestions } from '../data/quest
 import AiLoadingModal from './AiLoadingModal.vue';
 import PoseMiniWidget from './PoseMiniWidget.vue';
 
-interface Props {
+const props = defineProps<{
   roomName: string;
   timeRange: string;
   interviewers: string;
   candidates: string[];
   candidateIds: string[];
-}
-
-const props = withDefaults(defineProps<Props>(), {
-  roomName: '',
-  timeRange: '',
-  interviewers: '',
-  candidates: () => [],
-  candidateIds: () => []
-});
+  interviewerIds: number[];
+}>();
 
 const emit = defineEmits<{
-  (e: 'close'): void;
   (e: 'startSession'): void;
   (e: 'endSession'): void;
-  (e: 'toggleWebcam'): void;
+  (e: 'close'): void;
 }>();
 
 const router = useRouter();
@@ -121,9 +117,54 @@ const getQuestionsForCandidate = (candidateId: string): Question[] => {
   return getCandidateQuestions(candidateId);
 };
 
-const startSession = () => {
-  emit('startSession');
-};
+const startSession = async () => {
+  try {
+    console.log('면접 시작...', { candidateIds: props.candidateIds, interviewerIds: props.interviewerIds })
+    isAnalyzing.value = true
+
+    // FastAPI 서버에 면접 시작 요청
+    const response = await fetch('/api/v1/interview/start', {
+      method: 'POST',
+      headers: {
+        'Content-Type': 'application/json',
+      },
+      body: JSON.stringify({
+        interviewee_ids: props.candidateIds,
+        interviewer_ids: props.interviewerIds
+      })
+    })
+
+    if (!response.ok) {
+      const contentType = response.headers.get('content-type')
+      let errorMessage = '면접 시작 실패'
+      
+      try {
+        if (contentType && contentType.includes('application/json')) {
+          const errorData = await response.json()
+          errorMessage = errorData.detail || errorMessage
+        } else {
+          const text = await response.text()
+          console.error('서버 응답 (JSON이 아님):', text)
+          errorMessage = `서버 오류 (${response.status}): ${text}`
+        }
+      } catch (e) {
+        console.error('에러 응답 파싱 실패:', e)
+        errorMessage = `서버 오류 (${response.status})`
+      }
+      
+      throw new Error(errorMessage)
+    }
+
+    const result = await response.json()
+    console.log('면접 시작 성공:', result)
+    emit('startSession')  // 면접 시작 이벤트 발생
+    isAnalyzing.value = false
+  } catch (error: unknown) {
+    console.error('면접 시작 중 오류:', error)
+    isAnalyzing.value = false
+    alert('면접 시작 중 오류가 발생했습니다: ' + (error instanceof Error ? error.message : String(error)))
+  }
+}
 
 const endSession = async () => {
   try {
@@ -168,8 +209,9 @@ const endSession = async () => {
         tab: '0'
       }
     });
-  } catch (error) {
+  } catch (error: unknown) {
     console.error('면접 분석 중 오류 발생:', error);
+    alert('면접 분석 중 오류가 발생했습니다: ' + (error instanceof Error ? error.message : String(error)));
   } finally {
     isAnalyzing.value = false;
   }
diff --git a/client/src/components/InterviewSetup.vue b/client/src/components/InterviewSetup.vue
index 271cc51..2941144 100644
--- a/client/src/components/InterviewSetup.vue
+++ b/client/src/components/InterviewSetup.vue
@@ -218,6 +218,7 @@ const onStartInterview = () => {
       date: selectedDate.value,
       timeRange: selectedSchedule.value.timeRange,
       interviewers: selectedSchedule.value.interviewers.join(', '),
+      interviewerIds: JSON.stringify(selectedSchedule.value.interviewerIds),
       candidates: JSON.stringify(selectedSchedule.value.interviewees),
       candidateIds: JSON.stringify(selectedSchedule.value.interviewees)
     }
diff --git a/client/src/components/PoseMiniWidget.vue b/client/src/components/PoseMiniWidget.vue
index 0c3e3a7..136de77 100644
--- a/client/src/components/PoseMiniWidget.vue
+++ b/client/src/components/PoseMiniWidget.vue
@@ -17,50 +17,53 @@
 </template>
 
 <script setup>
-import { ref, onMounted, onBeforeUnmount } from 'vue'
+import { ref, onMounted, onBeforeUnmount, defineProps, watch } from 'vue'
 import * as faceapi from 'face-api.js'
-import {
-  FilesetResolver,
-  PoseLandmarker
-} from '@mediapipe/tasks-vision'
 
-// 1. WebSocket 연결
-const wsUrl = 'ws://localhost:9000'   // 서버 주소에 맞게 수정
-let ws = null
-function connectWebSocket() {
-  ws = new WebSocket(wsUrl)
-  ws.onopen = () => console.log('WebSocket 연결됨')
-  ws.onerror = err => console.error('WebSocket 에러:', err)
-  ws.onclose = () => console.log('WebSocket 연결 종료')
-}
-connectWebSocket()
+const props = defineProps({
+  intervieweeNames: {
+    type: Array,
+    required: true,
+    default: () => []
+  },
+  intervieweeIds: {
+    type: Array,
+    required: true,
+    default: () => []
+  }
+})
+
+// 녹음 관련 상태
+const mediaRecorder = ref(null)
+const audioChunks = ref([])
+const MOUTH_CLOSED_THRESHOLD = 3000 // 3초
 
 const video = ref(null)
 const canvas = ref(null)
-let poseLandmarker
 let active = true
 
-const LEFT_KNEE = 25, RIGHT_KNEE = 26
-const NOSE = 0, LEFT_SHOULDER = 11, RIGHT_SHOULDER = 12
-const POSE_FACE_LANDMARKS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
-let spreadCount = [0, 0]
-let shakeCount = [0, 0]
-let headDownCount = [0, 0]
-let prevKneeY = [
-  { left: null, right: null, max: null, min: null },
-  { left: null, right: null, max: null, min: null }
-]
-let lastCountedTime = 0
-
-const expList = ['미소', '무표정', '당황', '울상', '찡그림']
+// 얼굴표정 관련 상수
+const expList = ['미소', '무표정', '울상', '찡그림']
 const expKorean = {
-  happy: '미소', sad: '울상', angry: '찡그림', surprised: '당황',
-  neutral: '무표정', fearful: '불안', disgusted: '불쾌'
+  happy: '미소', sad: '울상', angry: '찡그림',
+  neutral: '무표정', disgusted: '불쾌'
 }
-let faceExpCount = [
-  Object.fromEntries(expList.map(e => [e, 0])),
-  Object.fromEntries(expList.map(e => [e, 0]))
-]
+
+// 각 면접자별 상태 관리
+const faceStates = ref([])
+
+// 면접자 이름이 변경될 때마다 상태 초기화
+watch(() => props.intervieweeNames, (newNames) => {
+  faceStates.value = newNames.map((name, index) => ({
+    name,
+    id: props.intervieweeIds[index],
+    speaking: false,
+    mouthClosedStartTime: null,
+    isRecording: false,  // 면접자별 녹음 상태 추가
+    expression: Object.fromEntries(expList.map(e => [e, 0])),
+    lastExpression: null
+  }))
+}, { immediate: true })
 
 function detectSpeaking(landmarks) {
   if (!landmarks || !landmarks.positions) return false
@@ -74,207 +77,290 @@ function detectSpeaking(landmarks) {
   return mouthOpen > 20 && mouthWidth > 25
 }
 
-function isValidKeypoint(lm) {
-  return (
-    lm &&
-    ((typeof lm.visibility === "number" && lm.visibility > 0.5) ||
-      (typeof lm.presence === "number" && lm.presence > 0.5) ||
-      (lm.visibility === undefined && lm.presence === undefined)) &&
-    lm.x >= 0.0 && lm.x <= 1.0 &&
-    lm.y >= 0.0 && lm.y <= 1.0
-  )
-}
-
-function analyzeLegByKnee(poseLandmarks, k) {
-  if (!isValidKeypoint(poseLandmarks[LEFT_KNEE]) || !isValidKeypoint(poseLandmarks[RIGHT_KNEE]))
-    return { kneeSpread: 0, kneeShakeAmp: 0, valid: false }
-
-  const kneeSpread = Math.abs(poseLandmarks[LEFT_KNEE].x - poseLandmarks[RIGHT_KNEE].x)
-  const avgKneeY = (poseLandmarks[LEFT_KNEE].y + poseLandmarks[RIGHT_KNEE].y) / 2
-
-  if (prevKneeY[k].min === null || prevKneeY[k].max === null) {
-    prevKneeY[k].min = avgKneeY
-    prevKneeY[k].max = avgKneeY
-  } else {
-    prevKneeY[k].min = Math.min(prevKneeY[k].min, avgKneeY)
-    prevKneeY[k].max = Math.max(prevKneeY[k].max, avgKneeY)
+async function startRecording(personIndex) {
+  console.log('startRecording 함수 호출됨, personIndex:', personIndex)
+  const state = faceStates.value[personIndex]
+  console.log('현재 faceState:', state)
+  
+  if (state.isRecording) {
+    console.log(`[녹음 시작 실패] ${state.name}님의 녹음이 이미 진행 중입니다.`)
+    return
+  }
+  
+  try {
+    console.log(`[녹음 준비] ${state.name}님의 녹음을 시작합니다...`)
+    const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
+    console.log(`[녹음 준비 완료] ${state.name}님의 오디오 스트림이 준비되었습니다.`)
+    
+    // WebM 형식으로 녹음 설정
+    const mimeType = 'audio/webm'
+    if (!MediaRecorder.isTypeSupported(mimeType)) {
+      console.error(`[녹음 시작 실패] ${mimeType} 형식이 지원되지 않습니다.`)
+      return
+    }
+    
+    mediaRecorder.value = new MediaRecorder(stream, {
+      mimeType: mimeType
+    })
+    audioChunks.value = []
+    
+    mediaRecorder.value.ondataavailable = (event) => {
+      audioChunks.value.push(event.data)
+      console.log(`[녹음 데이터] ${state.name}님의 녹음 데이터 청크 수신 (${audioChunks.value.length}개)`)
+    }
+    
+    mediaRecorder.value.onstop = async () => {
+      console.log(`[녹음 종료] ${state.name}님의 녹음이 종료되었습니다. 파일 변환 중...`)
+      const audioBlob = new Blob(audioChunks.value, { type: mimeType })
+      const timestamp = new Date().toISOString().replace(/[:.]/g, '-')
+      const fileName = `${state.id}_${timestamp}.webm`
+      
+      console.log(`[파일 생성] ${fileName} (${(audioBlob.size / 1024 / 1024).toFixed(2)}MB)`)
+      
+      const formData = new FormData()
+      formData.append('audio', audioBlob, fileName)
+      formData.append('interviewee_id', state.id)
+      
+      try {
+        console.log(`[업로드 시작] ${state.name}님의 녹음 파일을 서버로 전송합니다...`)
+        const response = await fetch('/api/v1/interview/stt/upload', {
+          method: 'POST',
+          body: formData
+        })
+        
+        if (!response.ok) {
+          const errorData = await response.json().catch(() => ({}))
+          throw new Error(`Upload failed: ${response.status} ${errorData.detail || response.statusText}`)
+        }
+        
+        const result = await response.json()
+        console.log(`[업로드 성공] ${state.name}님의 녹음 파일이 성공적으로 업로드되었습니다.`, result)
+        state.isRecording = false
+      } catch (error) {
+        console.error(`[업로드 실패] ${state.name}님의 녹음 파일 업로드 중 오류 발생:`, error.message)
+        state.isRecording = false
+      } finally {
+        if (mediaRecorder.value && mediaRecorder.value.stream) {
+          mediaRecorder.value.stream.getTracks().forEach(track => {
+            track.stop()
+            console.log(`[리소스 정리] ${state.name}님의 오디오 스트림 트랙이 정리되었습니다.`)
+          })
+        }
+      }
+    }
+    
+    mediaRecorder.value.start()
+    state.isRecording = true
+    console.log(`[녹음 시작] ${state.name}님의 녹음이 시작되었습니다.`)
+  } catch (error) {
+    console.error(`[녹음 시작 실패] ${state.name}님의 녹음 시작 중 오류 발생:`, error.message)
   }
-
-  const kneeShakeAmp = prevKneeY[k].max - prevKneeY[k].min
-  return { kneeSpread, kneeShakeAmp, valid: true }
 }
 
-function getSortedPersonIndexes(landmarksArr) {
-  if (!landmarksArr || landmarksArr.length === 0) return []
-  if (landmarksArr.length === 1) return [0]
-  let centers = []
-  for (let i = 0; i < landmarksArr.length; i++) {
-    const lmArr = landmarksArr[i]
-    if (!lmArr || lmArr.length === 0) continue
-    const avgX = lmArr.reduce((acc, l) => acc + l.x, 0) / lmArr.length
-    centers.push({ i, avgX })
+function stopRecording(personIndex) {
+  console.log('stopRecording 함수 호출됨, personIndex:', personIndex)
+  const state = faceStates.value[personIndex]
+  console.log('현재 faceState:', state)
+  
+  if (!state.isRecording) {
+    console.log(`[녹음 종료 실패] ${state.name}님의 녹음이 진행 중이 아닙니다.`)
+    return
+  }
+  
+  if (!mediaRecorder.value) {
+    console.error(`[녹음 종료 실패] ${state.name}님의 MediaRecorder가 초기화되지 않았습니다.`)
+    return
+  }
+  
+  try {
+    mediaRecorder.value.stop()
+    state.isRecording = false
+    console.log(`[녹음 종료 요청] ${state.name}님의 녹음 종료가 요청되었습니다.`)
+  } catch (error) {
+    console.error(`[녹음 종료 실패] ${state.name}님의 녹음 종료 중 오류 발생:`, error.message)
   }
-  centers.sort((a, b) => a.avgX - b.avgX)
-  return centers.map(c => c.i)
 }
 
 onMounted(async () => {
-  await faceapi.nets.tinyFaceDetector.loadFromUri('/models/tiny_face_detector')
-  await faceapi.nets.faceLandmark68Net.loadFromUri('/models/face_landmark_68')
-  await faceapi.nets.faceExpressionNet.loadFromUri('/models/face_expression')
-
-  while (!video.value) await new Promise(r => requestAnimationFrame(r))
-  const stream = await navigator.mediaDevices.getUserMedia({ video: { width: 1280, height: 720 } })
-  video.value.srcObject = stream
-  await new Promise(resolve => { video.value.onloadedmetadata = resolve })
-
-  const vision = await FilesetResolver.forVisionTasks('https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0/wasm')
-  poseLandmarker = await PoseLandmarker.createFromOptions(vision, {
-    baseOptions: {
-      modelAssetPath: 'https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_full/float16/1/pose_landmarker_full.task'
-    },
-    runningMode: 'VIDEO',
-    numPoses: 2,
-    outputSegmentationMasks: false
-  })
-
-  const analyze = async () => {
-    if (!active) return
-    const ctx = canvas.value.getContext('2d')
-    ctx.clearRect(0, 0, 1280, 720)
-    ctx.drawImage(video.value, 0, 0, 1280, 720)
-
-    const poses = poseLandmarker.detectForVideo(video.value, performance.now())
-    let poseIndexes = getSortedPersonIndexes(poses.landmarks)
-    if (poseIndexes.length > 2) poseIndexes = poseIndexes.slice(0, 2)
-
-    let detections = await faceapi.detectAllFaces(video.value, new faceapi.TinyFaceDetectorOptions())
-      .withFaceLandmarks()
-      .withFaceExpressions()
-    detections.sort((a, b) => a.detection.box.x - b.detection.box.x)
-    if (detections.length > 2) detections = detections.slice(0, 2)
+  console.log('=== PoseMiniWidget 컴포넌트 마운트 시작 ===')
+  console.log('면접자 목록:', props.intervieweeNames)
+  console.log('현재 faceStates:', faceStates.value)
 
-    const numPersons = Math.max(poseIndexes.length, detections.length)
-    if (numPersons === 0) {
-      requestAnimationFrame(analyze)
-      return
+  try {
+    console.log('face-api.js 모델 로딩 시작...')
+    
+    // 모델 로딩 전 상태 확인
+    if (!faceapi.nets.tinyFaceDetector.isLoaded) {
+      console.log('tinyFaceDetector 모델 로드 시작')
+      await faceapi.nets.tinyFaceDetector.loadFromUri('/models/tiny_face_detector')
+      console.log('tinyFaceDetector 모델 로드 완료')
+    } else {
+      console.log('tinyFaceDetector 모델이 이미 로드되어 있음')
+    }
+    
+    if (!faceapi.nets.faceLandmark68Net.isLoaded) {
+      console.log('faceLandmark68Net 모델 로드 시작')
+      await faceapi.nets.faceLandmark68Net.loadFromUri('/models/face_landmark_68')
+      console.log('faceLandmark68Net 모델 로드 완료')
+    } else {
+      console.log('faceLandmark68Net 모델이 이미 로드되어 있음')
+    }
+    
+    if (!faceapi.nets.faceExpressionNet.isLoaded) {
+      console.log('faceExpressionNet 모델 로드 시작')
+      await faceapi.nets.faceExpressionNet.loadFromUri('/models/face_expression')
+      console.log('faceExpressionNet 모델 로드 완료')
+    } else {
+      console.log('faceExpressionNet 모델이 이미 로드되어 있음')
     }
+    
+    console.log('모든 face-api.js 모델 로딩 완료')
 
-    // 얼굴 랜드마크/박스 시각화
-    if (detections.length === 1) {
-      const color = 'lime'
-      const det = detections[0]
-      for (const pt of det.landmarks.positions) {
-        ctx.beginPath()
-        ctx.arc(pt.x, pt.y, 2.2, 0, 2 * Math.PI)
-        ctx.fillStyle = color
-        ctx.fill()
+    // 비디오 엘리먼트 초기화
+    try {
+      while (!video.value) {
+        console.log('비디오 엘리먼트 대기 중...')
+        await new Promise(r => setTimeout(r, 100))  // requestAnimationFrame 대신 setTimeout 사용
       }
-      const box = det.detection.box
-      ctx.strokeStyle = color
-      ctx.lineWidth = 2
-      ctx.strokeRect(box.x, box.y, box.width, box.height)
-      ctx.font = 'bold 20px sans-serif'
-      ctx.fillStyle = color
-      ctx.fillText(`person1`, box.x, box.y - 8)
-    } else if (detections.length === 2) {
-      for (let k = 0; k < 2; k++) {
-        const color = k === 0 ? 'lime' : 'yellow'
-        const det = detections[k]
-        for (const pt of det.landmarks.positions) {
-          ctx.beginPath()
-          ctx.arc(pt.x, pt.y, 2.2, 0, 2 * Math.PI)
-          ctx.fillStyle = color
-          ctx.fill()
+      console.log('비디오 엘리먼트 준비 완료')
+
+      const stream = await navigator.mediaDevices.getUserMedia({ 
+        video: { 
+          width: 1280, 
+          height: 720,
+          facingMode: 'user'  // 전면 카메라 우선 사용
+        } 
+      })
+      console.log('카메라 스트림 획득 완료')
+      
+      video.value.srcObject = stream
+      await new Promise((resolve, reject) => {
+        if (!video.value) {
+          reject(new Error('비디오 엘리먼트가 없습니다.'))
+          return
         }
-        const box = det.detection.box
-        ctx.strokeStyle = color
-        ctx.lineWidth = 2
-        ctx.strokeRect(box.x, box.y, box.width, box.height)
-        ctx.font = 'bold 20px sans-serif'
-        ctx.fillStyle = color
-        ctx.fillText(`person${k + 1}`, box.x, box.y - 8)
-      }
+        video.value.onloadedmetadata = resolve
+        video.value.onerror = reject
+      })
+      console.log('비디오 메타데이터 로드 완료')
+
+    } catch (error) {
+      console.error('비디오 초기화 중 오류:', error)
+      throw error  // 상위에서 처리하도록 에러 전파
     }
 
-    // 포즈 랜드마크 시각화
-    for (let k = 0; k < poseIndexes.length; k++) {
-      const i = poseIndexes[k]
-      const landmarks = poses.landmarks[i]
-      if (!landmarks || landmarks.length < 20) continue
-      const color = k === 0 ? 'aqua' : 'orange'
-      for (let j = 0; j < landmarks.length; j++) {
-        ctx.beginPath()
-        ctx.arc(landmarks[j].x * 1280, landmarks[j].y * 720, 3, 0, 2 * Math.PI)
-        ctx.fillStyle = color
-        ctx.fill()
+    const analyze = async () => {
+      if (!active) {
+        console.log('analyze 함수 비활성화됨')
+        return
       }
-    }
 
-    // 1초마다 JSON 송신
-    if (Date.now() - lastCountedTime > 1000) {
-      for (let k = 0; k < numPersons; k++) {
-        let isSpeaking = false
-        if (detections[k]) {
-          isSpeaking = detectSpeaking(detections[k].landmarks)
+      try {
+        const ctx = canvas.value.getContext('2d')
+        ctx.clearRect(0, 0, 1280, 720)
+        ctx.drawImage(video.value, 0, 0, 1280, 720)
+
+        let detections = await faceapi.detectAllFaces(video.value, new faceapi.TinyFaceDetectorOptions())
+          .withFaceLandmarks()
+          .withFaceExpressions()
+        
+        // 면접자 수에 따라 감지된 얼굴 수 제한
+        detections = detections.slice(0, props.intervieweeNames.length)
+        detections.sort((a, b) => a.detection.box.x - b.detection.box.x)
+
+        if (detections.length > 0) {
+          console.log(`감지된 얼굴 수: ${detections.length}`)
         }
 
-        let poseObj = { leg_spread: 0, leg_shake: 0, head_down: 0 }
-        if (poseIndexes[k] !== undefined) {
-          const i = poseIndexes[k]
-          const landmarks = poses.landmarks[i]
-          if (landmarks && landmarks.length >= 20) {
-            const { kneeSpread, kneeShakeAmp, valid } = analyzeLegByKnee(landmarks, k)
-            if (valid && (kneeSpread > 0.21)) spreadCount[k] += 1
-            if (valid && (kneeShakeAmp > 0.04)) shakeCount[k] += 1
-            const nose = landmarks[NOSE]
-            const avgShoulderY = (landmarks[LEFT_SHOULDER].y + landmarks[RIGHT_SHOULDER].y) / 2
-            if (nose && nose.y > avgShoulderY + 0.04) headDownCount[k] += 1
-            poseObj = {
-              leg_spread: spreadCount[k],
-              leg_shake: shakeCount[k],
-              head_down: headDownCount[k]
+        // 얼굴 랜드마크/박스 시각화 및 상태 업데이트
+        for (let k = 0; k < detections.length; k++) {
+          const det = detections[k]
+          const color = k === 0 ? 'lime' : k === 1 ? 'yellow' : 'aqua'
+          
+          // 얼굴 랜드마크 시각화
+          for (const pt of det.landmarks.positions) {
+            ctx.beginPath()
+            ctx.arc(pt.x, pt.y, 2.2, 0, 2 * Math.PI)
+            ctx.fillStyle = color
+            ctx.fill()
+          }
+          
+          // 얼굴 박스 시각화
+          const box = det.detection.box
+          ctx.strokeStyle = color
+          ctx.lineWidth = 2
+          ctx.strokeRect(box.x, box.y, box.width, box.height)
+          
+          // 면접자 이름 표시
+          ctx.font = 'bold 20px sans-serif'
+          ctx.fillStyle = color
+          ctx.fillText(faceStates.value[k].name, box.x, box.y - 8)
+
+          // 입벌림 감지 및 녹음 처리
+          const isSpeaking = detectSpeaking(det.landmarks)
+          const faceState = faceStates.value[k]
+          
+          if (isSpeaking) {
+            if (!faceState.speaking) {
+              console.log(`[입벌림 감지] ${faceState.name}님이 말하기 시작했습니다.`)
+            }
+            faceState.speaking = true
+            faceState.mouthClosedStartTime = null
+            if (!faceState.isRecording) {
+              startRecording(k)
+            }
+          } else if (faceState.speaking) {
+            if (!faceState.mouthClosedStartTime) {
+              faceState.mouthClosedStartTime = Date.now()
+              console.log(`[입벌림 종료 감지] ${faceState.name}님이 말하기를 멈췄습니다. 3초 대기 중...`)
+            } else if (Date.now() - faceState.mouthClosedStartTime >= MOUTH_CLOSED_THRESHOLD) {
+              console.log(`[녹음 종료 조건 충족] ${faceState.name}님이 3초 동안 말하지 않았습니다.`)
+              faceState.speaking = false
+              faceState.mouthClosedStartTime = null
+              stopRecording(k)
             }
-            prevKneeY[k].min = prevKneeY[k].max = (landmarks[LEFT_KNEE].y + landmarks[RIGHT_KNEE].y) / 2
           }
-        }
-        if (!faceExpCount[k]) faceExpCount[k] = Object.fromEntries(expList.map(e => [e, 0]))
-        let faceExpTmp = Object.fromEntries(expList.map(e => [e, 0]))
-        if (detections[k]) {
-          const det = detections[k]
+
+          // 표정 감지 및 카운트
           const expLabel = Object.entries(det.expressions)
             .reduce((max, cur) => cur[1] > max[1] ? cur : max)[0]
-          const expKor = expKorean[expLabel] || expLabel
-          if (faceExpTmp[expKor] !== undefined) faceExpTmp[expKor] += 1
-          for (let key of expList) {
-            faceExpCount[k][key] += faceExpTmp[key]
+          const expKor = expKorean[expLabel]
+          if (expKor && expList.includes(expKor)) {
+            faceState.expression[expKor]++
+            faceState.lastExpression = expKor
           }
         }
 
-        // ----- JSON 송신 -----
-        const payload = {
-          person: k + 1,
-          speaking: isSpeaking,
-          pose: poseObj,
-          expression: Object.fromEntries(expList.map(e => [e, faceExpCount[k][e] || 0])),
-          timestamp: new Date().toISOString()
-        }
-        if (ws && ws.readyState === WebSocket.OPEN) {
-          ws.send(JSON.stringify(payload))
-        }
-        // 개발/테스트용 로그 출력
-        console.log(payload)
+      } catch (error) {
+        console.error('analyze 함수 실행 중 오류:', error)
       }
-      lastCountedTime = Date.now()
+
+      requestAnimationFrame(analyze)
     }
 
-    requestAnimationFrame(analyze)
+    console.log('analyze 함수 시작')
+    analyze()
+    console.log('=== PoseMiniWidget 컴포넌트 마운트 완료 ===')
+
+  } catch (error) {
+    console.error('PoseMiniWidget 초기화 중 오류 발생:', error)
+    // 사용자에게 오류 알림
+    alert('카메라 초기화 중 오류가 발생했습니다. 페이지를 새로고침하거나 카메라 권한을 확인해주세요.')
   }
-  analyze()
 })
 
 onBeforeUnmount(() => {
   active = false
-  if (ws) ws.close()
+  console.log('[컴포넌트 정리] PoseMiniWidget 컴포넌트를 정리합니다...')
+  
+  // 모든 면접자의 녹음 중지
+  faceStates.value.forEach((state, index) => {
+    if (state.isRecording) {
+      console.log(`[강제 종료] ${state.name}님의 녹음을 강제 종료합니다.`)
+      stopRecording(index)
+    }
+  })
+  
+  console.log('[컴포넌트 정리 완료] PoseMiniWidget 컴포넌트가 정리되었습니다.')
 })
 </script>
diff --git a/client/src/main.ts b/client/src/main.ts
index 837b2bd..7cc5bb4 100644
--- a/client/src/main.ts
+++ b/client/src/main.ts
@@ -29,6 +29,7 @@ const router = createRouter({
         roomName: route.query.roomName as string,
         timeRange: route.query.timeRange as string,
         interviewers: route.query.interviewers as string,
+        interviewerIds: JSON.parse(route.query.interviewerIds as string || '[]'),
         candidates: JSON.parse(route.query.candidates as string || '[]'),
         candidateIds: JSON.parse(route.query.candidateIds as string || '[]')
       })
diff --git a/client/vite.config.ts b/client/vite.config.ts
index f4a5df8..e37cca9 100644
--- a/client/vite.config.ts
+++ b/client/vite.config.ts
@@ -11,6 +11,13 @@ export default defineConfig({
   },
   server: {
     port: 3000,
-    open: true
+    open: true,
+    proxy: {
+      '/api': {
+        target: 'http://localhost:8000',
+        changeOrigin: true,
+        secure: false
+      }
+    }
   }
 }) 
\ No newline at end of file
-- 
2.39.5 (Apple Git-154)

