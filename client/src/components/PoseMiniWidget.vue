<template>
  <div style="position:relative; width:100%; height:100%;">
    <video
      ref="video"
      width="1280" height="720"
      autoplay
      muted
      playsinline
      style="position:absolute; top:0; left:0; width:100%; height:100%; object-fit:contain; z-index:1; background:#111;"
    ></video>
    <canvas
      ref="canvas"
      width="1280" height="720"
      style="position:absolute; top:0; left:0; width:100%; height:100%; z-index:100; pointer-events:none; background:transparent; border: 2px solid red;"
    ></canvas>
  </div>
</template>

<script setup>
import { ref, onMounted, onBeforeUnmount, defineProps, watch, defineEmits, defineExpose } from 'vue'
import * as faceapi from 'face-api.js'

const props = defineProps({
  intervieweeNames: {
    type: Array,
    required: true,
    default: () => []
  },
  intervieweeIds: {
    type: Array,
    required: true,
    default: () => []
  }
})

const emit = defineEmits(['updateNonverbalData'])

// Î∂ÄÎ™® Ïª¥Ìè¨ÎÑåÌä∏ÏóêÏÑú Ï†ëÍ∑º Í∞ÄÎä•ÌïòÎèÑÎ°ù ÎàÑÏ†Å Îç∞Ïù¥ÌÑ∞ ÎÖ∏Ï∂ú
defineExpose({
  getAccumulatedNonverbalData: () => accumulatedNonverbalData.value,
  getCurrentNonverbalData: () => nonverbalData.value,
  stopDetection: () => {
    active = false
    console.log('[Í∞êÏßÄ Ï§ëÎã®] PoseMiniWidget Í∞êÏßÄÍ∞Ä Ï§ëÎã®ÎêòÏóàÏäµÎãàÎã§.')
  }
})

// ÎÖπÏùå Í¥ÄÎ†® ÏÉÅÌÉú - Î©¥Ï†ëÏûêÎ≥Ñ Í∞úÎ≥Ñ Í¥ÄÎ¶¨
const recorderMap = ref({})  // { [id]: { mediaRecorder, audioChunks, stream } }
const MOUTH_CLOSED_THRESHOLD = 5000 // 5Ï¥à

const video = ref(null)
const canvas = ref(null)
let active = true

// ÏñºÍµ¥ÌëúÏ†ï Í¥ÄÎ†® ÏÉÅÏàò
const expList = ['ÎØ∏ÏÜå', 'Î¨¥ÌëúÏ†ï', 'Ïö∏ÏÉÅ', 'Ï∞°Í∑∏Î¶º']
const expKorean = {
  happy: 'ÎØ∏ÏÜå', sad: 'Ïö∏ÏÉÅ', angry: 'Ï∞°Í∑∏Î¶º',
  neutral: 'Î¨¥ÌëúÏ†ï', disgusted: 'Î∂àÏæå'
}

// Í∞Å Î©¥Ï†ëÏûêÎ≥Ñ ÏÉÅÌÉú Í¥ÄÎ¶¨
const faceStates = ref([])

// ÎπÑÏñ∏Ïñ¥Ï†Å Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•ÏÜå
const nonverbalData = ref({})

// Î©¥Ï†ë Ï¢ÖÎ£å Ïãú ÎàÑÏ†Å Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•ÏÜå
const accumulatedNonverbalData = ref({})  // { [id]: { facial_expression_history: [], posture_history: [], ... } }

// 1Ï¥àÎßàÎã§ Îç∞Ïù¥ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Ï†ÑÏÜ°
let updateInterval = null

// Î©¥Ï†ëÏûê Ïù¥Î¶ÑÏù¥ Î≥ÄÍ≤ΩÎê† ÎïåÎßàÎã§ ÏÉÅÌÉú Ï¥àÍ∏∞Ìôî
watch(() => props.intervieweeNames, (newNames) => {
  faceStates.value = newNames.map((name, index) => {
    const id = props.intervieweeIds[index]
    nonverbalData.value[id] = {
      posture: { upright: 0, leaning: 0, slouching: 0 },
      facial_expression: { smile: 0, neutral: 0, frown: 0, angry: 0 },
      gaze: 0,
      gesture: 0,
      timestamp: Date.now()
    }
    
    // ÎàÑÏ†Å Îç∞Ïù¥ÌÑ∞ Ï¥àÍ∏∞Ìôî
    accumulatedNonverbalData.value[id] = {
      facial_expression_history: [],
      posture_history: [],
      gaze_history: [],
      gesture_history: [],
      start_time: Date.now()
    }
    
    return {
      name,
      id,
      speaking: false,
      mouthClosedStartTime: null,
      isRecording: false,
      expression: Object.fromEntries(expList.map(e => [e, 0])),
      expressionTotal: 0, // Ï¥ù ÌîÑÎ†àÏûÑ Ïàò
      lastExpression: null
    }
  })
}, { immediate: true })

function detectSpeaking(landmarks) {
  if (!landmarks || !landmarks.positions) return false
  const topLip = landmarks.positions[62]
  const bottomLip = landmarks.positions[66]
  const leftMouth = landmarks.positions[60]
  const rightMouth = landmarks.positions[64]
  if (!topLip || !bottomLip || !leftMouth || !rightMouth) return false
  const mouthOpen = Math.abs(topLip.y - bottomLip.y)
  const mouthWidth = Math.abs(leftMouth.x - rightMouth.x)
  return mouthOpen > 10 && mouthWidth > 20
}

async function startRecording(personIndex) {
  const state = faceStates.value[personIndex]
  
  if (state.isRecording) {
    console.log(`[ÎÖπÏùå ÏãúÏûë Ïã§Ìå®] ${state.name}ÎãòÏùò ÎÖπÏùåÏù¥ Ïù¥ÎØ∏ ÏßÑÌñâ Ï§ëÏûÖÎãàÎã§.`)
    return
  }
  
  try {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
    
    // WebM ÌòïÏãùÏúºÎ°ú ÎÖπÏùå ÏÑ§Ï†ï
    const mimeType = 'audio/webm'
    if (!MediaRecorder.isTypeSupported(mimeType)) {
      console.error(`[ÎÖπÏùå ÏãúÏûë Ïã§Ìå®] ${mimeType} ÌòïÏãùÏù¥ ÏßÄÏõêÎêòÏßÄ ÏïäÏäµÎãàÎã§.`)
      return
    }
    
    const recorder = new MediaRecorder(stream, {
      mimeType: mimeType
    })
    const audioChunks = []
    
    recorder.ondataavailable = (event) => {
      audioChunks.push(event.data)
    }
    
    recorder.onstop = async () => {
      const audioBlob = new Blob(audioChunks, { type: mimeType })
      const timestamp = new Date().toISOString().replace(/[:.]/g, '-')
      const fileName = `${state.id}_${timestamp}.webm`
      
      const formData = new FormData()
      formData.append('audio', audioBlob, fileName)
      formData.append('interviewee_id', state.id.toString())
      
      try {
        const response = await fetch('http//:localhost:8000/api/v1/stt/upload', {
          method: 'POST',
          body: formData
        })
        
        if (!response.ok) {
          const errorData = await response.json().catch(() => ({}))
          throw new Error(`Upload failed: ${response.status} ${errorData.detail || response.statusText}`)
        }
        
        const result = await response.json()
        console.log(`[ÏóÖÎ°úÎìú ÏÑ±Í≥µ] ${state.name}ÎãòÏùò ÎÖπÏùå ÌååÏùºÏù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏóÖÎ°úÎìúÎêòÏóàÏäµÎãàÎã§.`)
        state.isRecording = false
      } catch (error) {
        console.error(`[ÏóÖÎ°úÎìú Ïã§Ìå®] ${state.name}ÎãòÏùò ÎÖπÏùå ÌååÏùº ÏóÖÎ°úÎìú Ï§ë Ïò§Î•ò Î∞úÏÉù:`, error.message)
        state.isRecording = false
      } finally {
        if (recorder && recorder.stream) {
          recorder.stream.getTracks().forEach(track => {
            track.stop()
          })
        }
      }
    }
    
    recorder.start()
    state.isRecording = true
    console.log(`[ÎÖπÏùå ÏãúÏûë] ${state.name}ÎãòÏùò ÎÖπÏùåÏù¥ ÏãúÏûëÎêòÏóàÏäµÎãàÎã§.`)

    recorderMap.value[state.id] = { mediaRecorder: recorder, audioChunks, stream }
  } catch (error) {
    console.error(`[ÎÖπÏùå ÏãúÏûë Ïã§Ìå®] ${state.name}ÎãòÏùò ÎÖπÏùå ÏãúÏûë Ï§ë Ïò§Î•ò Î∞úÏÉù:`, error.message)
  }
}

function stopRecording(personIndex) {
  const state = faceStates.value[personIndex]
  
  if (!state.isRecording) {
    console.log(`[ÎÖπÏùå Ï¢ÖÎ£å Ïã§Ìå®] ${state.name}ÎãòÏùò ÎÖπÏùåÏù¥ ÏßÑÌñâ Ï§ëÏù¥ ÏïÑÎãôÎãàÎã§.`)
    return
  }
  
  if (!recorderMap.value[state.id]) {
    console.error(`[ÎÖπÏùå Ï¢ÖÎ£å Ïã§Ìå®] ${state.name}ÎãòÏùò MediaRecorderÍ∞Ä Ï¥àÍ∏∞ÌôîÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.`)
    return
  }
  
  try {
    const recorder = recorderMap.value[state.id].mediaRecorder
    recorder.stop()
    state.isRecording = false
    console.log(`[ÎÖπÏùå Ï¢ÖÎ£å] ${state.name}ÎãòÏùò ÎÖπÏùåÏù¥ Ï¢ÖÎ£åÎêòÏóàÏäµÎãàÎã§.`)
  } catch (error) {
    console.error(`[ÎÖπÏùå Ï¢ÖÎ£å Ïã§Ìå®] ${state.name}ÎãòÏùò ÎÖπÏùå Ï¢ÖÎ£å Ï§ë Ïò§Î•ò Î∞úÏÉù:`, error.message)
  }
}

onMounted(async () => {
  console.log('=== PoseMiniWidget Ïª¥Ìè¨ÎÑåÌä∏ ÎßàÏö¥Ìä∏ ÏãúÏûë ===')

  try {
    console.log('face-api.js Î™®Îç∏ Î°úÎî© ÏãúÏûë...')
    
    // Î™®Îç∏ Î°úÎî© Ï†Ñ ÏÉÅÌÉú ÌôïÏù∏
    if (!faceapi.nets.tinyFaceDetector.isLoaded) {
      await faceapi.nets.tinyFaceDetector.loadFromUri('/models/tiny_face_detector')
    }
    
    if (!faceapi.nets.faceLandmark68Net.isLoaded) {
      await faceapi.nets.faceLandmark68Net.loadFromUri('/models/face_landmark_68')
    }
    
    if (!faceapi.nets.faceExpressionNet.isLoaded) {
      await faceapi.nets.faceExpressionNet.loadFromUri('/models/face_expression')
    }
    
    console.log('Î™®Îì† face-api.js Î™®Îç∏ Î°úÎî© ÏôÑÎ£å')

    // ÎπÑÎîîÏò§ ÏóòÎ¶¨Î®ºÌä∏ Ï¥àÍ∏∞Ìôî
    try {
      while (!video.value) {
        await new Promise(r => setTimeout(r, 100))
      }

      const stream = await navigator.mediaDevices.getUserMedia({ 
        video: { 
          width: 1280, 
          height: 720,
          facingMode: 'user'
        } 
      })
      
      video.value.srcObject = stream
      
      // ÎπÑÎîîÏò§ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î°úÎìú ÏôÑÎ£å ÌõÑ Ï∫îÎ≤ÑÏä§ ÌÅ¨Í∏∞ ÎèôÍ∏∞Ìôî
      await new Promise((resolve, reject) => {
        if (!video.value) {
          reject(new Error('ÎπÑÎîîÏò§ ÏóòÎ¶¨Î®ºÌä∏Í∞Ä ÏóÜÏäµÎãàÎã§.'))
          return
        }
        video.value.onloadedmetadata = () => {
          console.log('ÎπÑÎîîÏò§ Ïã§Ï†ú Ìï¥ÏÉÅÎèÑ:', video.value.videoWidth, video.value.videoHeight)
          // üí° Ïã§Ï†ú ÎπÑÎîîÏò§ Ìï¥ÏÉÅÎèÑÎ•º Í∏∞Î∞òÏúºÎ°ú canvas Ìï¥ÏÉÅÎèÑ ÏÑ§Ï†ï (Ïä§ÏºÄÏùºÎßÅ Î¨∏Ï†ú Ìï¥Í≤∞)
          const width = video.value.videoWidth
          const height = video.value.videoHeight
          canvas.value.width = width
          canvas.value.height = height
          
          // Canvas Ïä§ÌÉÄÏùº ÎèôÏ†Å ÏÑ§Ï†ï
          canvas.value.style.zIndex = '100'
          canvas.value.style.position = 'absolute'
          canvas.value.style.top = '0'
          canvas.value.style.left = '0'
          canvas.value.style.width = '100%'
          canvas.value.style.height = '100%'
          canvas.value.style.pointerEvents = 'none'
          canvas.value.style.background = 'transparent'
          
          console.log(`Canvas Ìï¥ÏÉÅÎèÑ ÎèôÍ∏∞Ìôî ÏôÑÎ£å: ${width}x${height}`)
          
          // ÌÖåÏä§Ìä∏Ïö© Îπ®Í∞Ñ ÏÇ¨Í∞ÅÌòï Í∑∏Î¶¨Í∏∞
          const ctx = canvas.value.getContext('2d')
          ctx.fillStyle = 'red'
          ctx.fillRect(20, 20, 50, 50)
          
          resolve()
        }
        video.value.onerror = reject
      })

    } catch (error) {
      console.error('ÎπÑÎîîÏò§ Ï¥àÍ∏∞Ìôî Ï§ë Ïò§Î•ò:', error)
      throw error
    }

    const analyze = async () => {
      if (!active) {
        return
      }

      try {
        const ctx = canvas.value.getContext('2d')
        const width = canvas.value.width
        const height = canvas.value.height
        
        ctx.clearRect(0, 0, width, height)
        ctx.drawImage(video.value, 0, 0, width, height)

        // ÌÖåÏä§Ìä∏Ïö© Ï†ê Ï∞çÍ∏∞ (Í≥ÑÏÜç Í∑∏Î¶¨Í∏∞)
        ctx.fillStyle = 'red'
        ctx.fillRect(10, 10, 10, 10)
        ctx.fillRect(width - 20, height - 20, 10, 10) // Ïö∞ÌïòÎã®ÏóêÎèÑ Ï†ê Ï∞çÍ∏∞

        let detections = await faceapi.detectAllFaces(video.value, new faceapi.TinyFaceDetectorOptions())
          .withFaceLandmarks()
          .withFaceExpressions()
        
        // Î©¥Ï†ëÏûê ÏàòÏóê Îî∞Îùº Í∞êÏßÄÎêú ÏñºÍµ¥ Ïàò Ï†úÌïú
        detections = detections.slice(0, props.intervieweeNames.length)
        detections.sort((a, b) => a.detection.box.x - b.detection.box.x)

        // ÏñºÍµ¥ ÎûúÎìúÎßàÌÅ¨/Î∞ïÏä§ ÏãúÍ∞ÅÌôî Î∞è ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏
        for (let k = 0; k < detections.length; k++) {
          const det = detections[k]
          const color = k === 0 ? 'lime' : k === 1 ? 'yellow' : 'aqua'
          
          // faceStates ÏïàÏ†ÑÏÑ± Ï≤¥ÌÅ¨
          if (!faceStates.value[k]) {
            continue
          }
          const faceState = faceStates.value[k]
          
          // ÏñºÍµ¥ ÎûúÎìúÎßàÌÅ¨ ÏãúÍ∞ÅÌôî
          for (const pt of det.landmarks.positions) {
            ctx.beginPath()
            ctx.arc(pt.x, pt.y, 2.2, 0, 2 * Math.PI)
            ctx.fillStyle = color
            ctx.fill()
          }
          
          // ÏñºÍµ¥ Î∞ïÏä§ ÏãúÍ∞ÅÌôî
          const box = det.detection.box
          ctx.strokeStyle = color
          ctx.lineWidth = 2
          ctx.strokeRect(
            box.x,
            box.y,
            box.width,
            box.height
          )
          
          // Î©¥Ï†ëÏûê Ïù¥Î¶Ñ ÌëúÏãú
          ctx.font = 'bold 20px sans-serif'
          ctx.fillStyle = color
          ctx.fillText(
            faceState.name,
            box.x,
            box.y - 8
          )

          // ÏûÖÎ≤åÎ¶º Í∞êÏßÄ Î∞è ÎÖπÏùå Ï≤òÎ¶¨
          const isSpeaking = detectSpeaking(det.landmarks)
          
          if (isSpeaking) {
            if (!faceState.speaking) {
              console.log(`[ÏûÖÎ≤åÎ¶º Í∞êÏßÄ] ${faceState.name}ÎãòÏù¥ ÎßêÌïòÍ∏∞ ÏãúÏûëÌñàÏäµÎãàÎã§.`)
            }
            faceState.speaking = true
            faceState.mouthClosedStartTime = null
            if (!faceState.isRecording) {
              startRecording(k)
            }
          } else if (faceState.speaking) {
            if (!faceState.mouthClosedStartTime) {
              faceState.mouthClosedStartTime = Date.now()
            } else if (Date.now() - faceState.mouthClosedStartTime >= MOUTH_CLOSED_THRESHOLD) {
              console.log(`[ÎÖπÏùå Ï¢ÖÎ£å] ${faceState.name}ÎãòÏù¥ 3Ï¥à ÎèôÏïà ÎßêÌïòÏßÄ ÏïäÏïòÏäµÎãàÎã§.`)
              faceState.speaking = false
              faceState.mouthClosedStartTime = null
              stopRecording(k)
            }
          }

          // ÌëúÏ†ï Í∞êÏßÄ Î∞è Ïπ¥Ïö¥Ìä∏
          const expLabel = Object.entries(det.expressions)
            .reduce((max, cur) => cur[1] > max[1] ? cur : max)[0]
          const expKor = expKorean[expLabel]
          if (expKor && expList.includes(expKor)) {
            faceState.expression[expKor]++
            faceState.expressionTotal++
            faceState.lastExpression = expKor
          }
        }

      } catch (error) {
        console.error('analyze Ìï®Ïàò Ïã§Ìñâ Ï§ë Ïò§Î•ò:', error)
      }

      requestAnimationFrame(analyze)
    }

    analyze()
    console.log('=== PoseMiniWidget Ïª¥Ìè¨ÎÑåÌä∏ ÎßàÏö¥Ìä∏ ÏôÑÎ£å ===')

  } catch (error) {
    console.error('PoseMiniWidget Ï¥àÍ∏∞Ìôî Ï§ë Ïò§Î•ò Î∞úÏÉù:', error)
    // ÏÇ¨Ïö©ÏûêÏóêÍ≤å Ïò§Î•ò ÏïåÎ¶º
    alert('Ïπ¥Î©îÎùº Ï¥àÍ∏∞Ìôî Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. ÌéòÏù¥ÏßÄÎ•º ÏÉàÎ°úÍ≥†Ïπ®ÌïòÍ±∞ÎÇò Ïπ¥Î©îÎùº Í∂åÌïúÏùÑ ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî.')
  }

  // 1Ï¥àÎßàÎã§ Îç∞Ïù¥ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Ï†ÑÏÜ°
  updateInterval = setInterval(() => {
    const currentData = {}
    faceStates.value.forEach((state, index) => {
      const id = props.intervieweeIds[index]
      // ÎàÑÏ†ÅÍ∞í Ìï©ÏÇ∞
      const acc = accumulatedNonverbalData.value[id]
      // ÌëúÏ†ï ÎàÑÏ†Å Ìï©ÏÇ∞
      const expHistory = acc?.facial_expression_history || []
      const sumExp = expHistory.reduce((acc, cur) => {
        acc.smile += cur.smile || 0
        acc.neutral += cur.neutral || 0
        acc.frown += cur.frown || 0
        acc.angry += cur.angry || 0
        return acc
      }, { smile: 0, neutral: 0, frown: 0, angry: 0 })
      // ÏûêÏÑ∏ ÎàÑÏ†Å Ìï©ÏÇ∞
      const postureHistory = acc?.posture_history || []
      const sumPosture = postureHistory.reduce((acc, cur) => {
        acc.upright += cur.upright || 0
        acc.leaning += cur.leaning || 0
        acc.slouching += cur.slouching || 0
        return acc
      }, { upright: 0, leaning: 0, slouching: 0 })
      // ÎßàÏßÄÎßâ timestamp
      const lastTimestamp = expHistory.length > 0 ? expHistory[expHistory.length-1].timestamp : Date.now()
      currentData[id] = {
        posture: sumPosture,
        facial_expression: sumExp,
        gaze: 0, // ÎàÑÏ†Å ÏãúÏÑ†/Ï†úÏä§Ï≤ò ÌïÑÏöîÏãú Ï∂îÍ∞Ä
        gesture: 0,
        timestamp: lastTimestamp
      }
    })
    nonverbalData.value = currentData
    emit('updateNonverbalData', currentData)
  }, 1000)
})

onBeforeUnmount(() => {
  active = false
  console.log('[Ïª¥Ìè¨ÎÑåÌä∏ Ï†ïÎ¶¨] PoseMiniWidget Ïª¥Ìè¨ÎÑåÌä∏Î•º Ï†ïÎ¶¨Ìï©ÎãàÎã§...')
  
  // Î™®Îì† Î©¥Ï†ëÏûêÏùò ÎÖπÏùå Ï§ëÏßÄ Î∞è Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨
  Object.entries(recorderMap.value).forEach(([id, recorderData]) => {
    if (recorderData.mediaRecorder && recorderData.mediaRecorder.state !== 'inactive') {
      console.log(`[Í∞ïÏ†ú Ï¢ÖÎ£å] Î©¥Ï†ëÏûê ID ${id}Ïùò ÎÖπÏùåÏùÑ Í∞ïÏ†ú Ï¢ÖÎ£åÌï©ÎãàÎã§.`)
      try {
        recorderData.mediaRecorder.stop()
      } catch (error) {
        console.warn(`[Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨] Î©¥Ï†ëÏûê ID ${id}Ïùò ÎÖπÏùå Ï¢ÖÎ£å Ï§ë Ïò§Î•ò:`, error.message)
      }
    }
    
    // Ïä§Ìä∏Î¶º Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨
    if (recorderData.stream) {
      recorderData.stream.getTracks().forEach(track => {
        track.stop()
      })
    }
  })
  
  // recorderMap Ï¥àÍ∏∞Ìôî
  recorderMap.value = {}
  
  console.log('[Ïª¥Ìè¨ÎÑåÌä∏ Ï†ïÎ¶¨ ÏôÑÎ£å] PoseMiniWidget Ïª¥Ìè¨ÎÑåÌä∏Í∞Ä Ï†ïÎ¶¨ÎêòÏóàÏäµÎãàÎã§.')

  if (updateInterval) {
    clearInterval(updateInterval)
  }
})
</script>
